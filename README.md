# Artificial-Intelligence-Alignment-Resources

## AI Aligenment

**Large language model alignment: A survey.**<br>
*T Shen, R Jin, Y Huang, C Liu, W Dong, Z Guo, X Wu, Y Liu, D Xiong.*<br>
arXiv:2309.15025, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.15025)]

**Ai alignment: A comprehensive survey.**<br>
*J Ji, T Qiu, B Chen, B Zhang, H Lou, K Wang, Y Duan, Z He, J Zhou, Z Zhang, F Zeng, KY Ng, et al.*<br>
arXiv:2310.19852, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.19852)]
[[Homepage](https://alignmentsurvey.com/)]

**A Moral Imperative: The Need for Continual Superalignment of Large Language Models.**<br>
*G Puthumanaillam, M Vora, P Thangeda, M Ornik.*<br>
arXiv:2403.14683, 2024.
[[ArXiv](https://arxiv.org/pdf/2403.14683)]

## Al Security

**Safeguarding Large Language Models: A Survey.**<br>
*Y Dong, R Mu, Y Zhang, S Sun, T Zhang, C Wu, G Jin, Y Qi, J Hu, J Meng, S Bensalem, et al.*<br>
arXiv:2406.02622, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.02622)]

## PEFT

**Parameter-efficient fine-tuning of large-scale pre-trained language models.**<br>
*N Ding, Y Qin, G Yang, F Wei, Z Yang, Y Su, S Hu, Y Chen, CM Chan, W Chen, J Yi, W Zhao, et al.*<br>
Nature Machine Intelligence, 2023.
[[Paper](https://www.nature.com/articles/s42256-023-00626-4)]

**Lora: Low-rank adaptation of large language models.**<br>
*EJ Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen.*<br>
arXiv:2106.09685, 2021.
[[Paper](https://arxiv.org/pdf/2106.09685))]

**Longlora: Efficient fine-tuning of long-context large language models.**<br>
*Y Chen, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia.*<br>
arXiv:2309.12307, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.12307)]
[[Github](https://github.com/dvlab-research/LongLoRA)]
