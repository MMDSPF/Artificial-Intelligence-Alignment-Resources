# Artificial-Intelligence-Alignment-Resources

## Finetuning

### PEFT

**Parameter-efficient fine-tuning of large-scale pre-trained language models.**<br>
*N Ding, Y Qin, G Yang, F Wei, Z Yang, Y Su, S Hu, Y Chen, CM Chan, W Chen, J Yi, W Zhao, et al.*<br>
Nature Machine Intelligence, 2023.
[[Paper](https://www.nature.com/articles/s42256-023-00626-4)]

**Lora: Low-rank adaptation of large language models.**<br>
*EJ Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen.*<br>
arXiv:2106.09685, 2021.
[[Paper](https://arxiv.org/pdf/2106.09685))]

**Longlora: Efficient fine-tuning of long-context large language models.**<br>
*Y Chen, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia.*<br>
arXiv:2309.12307, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.12307)]
[[Github](https://github.com/dvlab-research/LongLoRA)]

## Review

**Large language model alignment: A survey.**<br>
*T Shen, R Jin, Y Huang, C Liu, W Dong, Z Guo, X Wu, Y Liu, D Xiong.*<br>
arXiv:2309.15025, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.15025)]

**Ai alignment: A comprehensive survey.**<br>
*J Ji, T Qiu, B Chen, B Zhang, H Lou, K Wang, Y Duan, Z He, J Zhou, Z Zhang, F Zeng, KY Ng, et al.*<br>
arXiv:2310.19852, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.19852)]
[[Homepage](https://alignmentsurvey.com/)]

**Aligning large language models with human: A survey.**<br>
*Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang, L Shang, X Jiang, Q Liu.*<br>
arXiv:2307.12966, 2023.
[[ArXiv](https://arxiv.org/pdf/2307.12966)]
[[Homepage](https://github.com/GaryYufei/AlignLLMHumanSurvey/)]

## Preference Aligenment

**Preference ranking optimization for human alignment.**<br>
*F Song, B Yu, M Li, H Yu, F Huang, Y Li, et al.*<br>
AAAI, 2024.
[[Paper](https://ojs.aaai.org/index.php/AAAI/article/download/29865/31509)]

**Aligner: Achieving efficient alignment through weak-to-strong correction.**<br>
*J Ji, B Chen, H Lou, D Hong, B Zhang, X Pan, et al.*<br>
arXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2402.02416)]
[[HomePage](https://aligner2024.github.io/)]

## Value Aligenment

**Aligning ai with shared human values.**<br>
*D Hendrycks, C Burns, S Basart, A Critch, J Li, D Song, J Steinhardt.*<br>
ICLR, 2021.
[[ArXiv](https://arxiv.org/pdf/2008.02275)]

**A Moral Imperative: The Need for Continual Superalignment of Large Language Models.**<br>
*G Puthumanaillam, M Vora, P Thangeda, M Ornik.*<br>
arXiv:2403.14683, 2024.
[[ArXiv](https://arxiv.org/pdf/2403.14683)]

## Security Aligenment

**A survey of safety and trustworthiness of large language models through the lens of verification and validation.**<br>
*X Huang, W Ruan, W Huang, G **, Y Dong, C Wu, S Bensalem, R Mu, Y Qi, X Zhao, K Cai, et al.*<br>
arxiv:2305.11391, 2023.
[[ArXiv](https://arxiv.org/pdf/2305.11391)]

**A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.**<br>
*Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang.*<br>
High-Confidence Computing, 2024.
[[Paper](https://www.sciencedirect.com/science/article/pii/S266729522400014X)]

**Safeguarding Large Language Models: A Survey.**<br>
*Y Dong, R Mu, Y Zhang, S Sun, T Zhang, C Wu, G Jin, Y Qi, J Hu, J Meng, S Bensalem, et al.*<br>
arXiv:2406.02622, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.02622)]
